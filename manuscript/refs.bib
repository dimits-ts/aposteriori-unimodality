@article{ChenFengYi2017,
  title={A general introduction to adjustment for multiple comparisons},
  author={Chen, S.Y. and Feng, Z. and Yi, X.},
  journal={Journal of Thoracic Disease},
  volume={9},
  number={6},
  pages={1725--1729},
  year={2017},
  note={doi: 10.21037/jtd.2017.05.34; PMID: 28740688; PMCID: PMC5506159},
}

@article{holms,
	ISSN = {03036898, 14679469},
	URL = {http://www.jstor.org/stable/4615733},
	abstract = {This paper presents a simple and widely applicable multiple test procedure of the sequentially rejective type, i.e. hypotheses are rejected one at a time until no further rejections can be done. It is shown that the test has a prescribed level of significance protection against error of the first kind for any combination of true hypotheses. The power properties of the test and a number of possible applications are also discussed.},
	author = {Sture Holm},
	journal = {Scandinavian Journal of Statistics},
	number = {2},
	pages = {65--70},
	publisher = {[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
	title = {A Simple Sequentially Rejective Multiple Test Procedure},
	urldate = {2025-07-07},
	volume = {6},
	year = {1979}
}

@inproceedings{pavlopoulos-likas-2024,
    title = "Polarized Opinion Detection Improves the Detection of Toxic Language",
    author = "Pavlopoulos, John  and
      Likas, Aristidis",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.117/",
    pages = "1946--1958",
    abstract = "Distance from unimodality (DFU) has been found to correlate well with human judgment for the assessment of polarized opinions. However, its un-normalized nature makes it less intuitive and somewhat difficult to exploit in machine learning (e.g., as a supervised signal). In this work a normalized version of this measure, called nDFU, is proposed that leads to better assessment of the degree of polarization. Then, we propose a methodology for K-class text classification, based on nDFU, that exploits polarized texts in the dataset. Such polarized instances are assigned to a separate K+1 class, so that a K+1-class classifier is trained. An empirical analysis on three datasets for abusive language detection, shows that nDFU can be used to model polarized annotations and prevent them from harming the classification performance. Finally, we further exploit nDFU to specify conditions that could explain polarization given a dimension and present text examples that polarized the annotators when the dimension was gender and race. Our code is available at https://github.com/ipavlopoulos/ndfu."
}

@inproceedings{sap-etal-2022-annotators,
    title = "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    author = "Sap, Maarten  and
      Swayamdipta, Swabha  and
      Vianna, Laura  and
      Zhou, Xuhui  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.431/",
    doi = "10.18653/v1/2022.naacl-main.431",
    pages = "5884--5906",
    abstract = "The perceived toxicity of language can vary based on someone{'}s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system{'}s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection."
}

@inproceedings{kumar-et-al-2021,
    author = {Kumar, Deepak and Kelley, Patrick Gage and Consolvo, Sunny and Mason, Joshua and Bursztein, Elie and Durumeric, Zakir and Thomas, Kurt and Bailey, Michael},
    title = {Designing toxic content classification for a diversity of perspectives},
    year = {2021},
    isbn = {978-1-939133-25-0},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {In this work, we demonstrate how existing classifiers for identifying toxic comments online fail to generalize to the diverse concerns of Internet users. We survey 17,280 participants to understand how user expectations for what constitutes toxic content differ across demographics, beliefs, and personal experiences. We find that groups historically at-risk of harassment--such as people who identify as LGBTQ+ or young adults--are more likely to to flag a random comment drawn from Reddit, Twitter, or 4chan as toxic, as are people who have personally experienced harassment in the past. Based on our findings, we show how current one-size-fits-all toxicity classification algorithms, like the Perspective API from Jigsaw, can improve in accuracy by 86\% on average through personalized model tuning. Ultimately, we highlight current pitfalls and new design directions that can improve the equity and efficacy of toxic content classifiers for all users.},
    booktitle = {Proceedings of the Seventeenth USENIX Conference on Usable Privacy and Security},
    articleno = {16},
    numpages = {19},
    series = {SOUPS'21}
}

@misc{tsirmpas2025scalableevaluationonlinefacilitation,
      title={Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions}, 
      author={Dimitris Tsirmpas and Ion Androutsopoulos and John Pavlopoulos},
      year={2025},
      eprint={2503.16505},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2503.16505}, 
}

@article{trafimow2018manipulating,
  title     = {Manipulating the Alpha Level Cannot Cure Significance Testing},
  author    = {Trafimow, David and Amrhein, Valentin and Areshenkoff, Cameron N. and Barrera-Causil, Carlos J. and Beh, Eric J. and Bilgiç, Yusuf Kaan and Bono, Roser and Bradley, Michael T. and Briggs, William M. and Cepeda-Freyre, Horacio A. and Chaigneau, Sergio E. and Ciocca, Daniel R. and Correa, Juan C. and Cousineau, Denis and de Boer, Menno R. and Dhar, Shashi S. and Dolgov, Igor and Gómez-Benito, Juana and Grendar, Marian and Grice, James W. and Guerrero-Gimenez, Manuel E. and Gutiérrez, Adriana and Huedo-Medina, Tania B. and Jaffe, Klaus and Janyan, Armina and Karimnezhad, Arash and Korner-Nievergelt, Fränzi and Kosugi, Kazuki and Lachmair, Martin and Ledesma, Ruben D. and Limongi, Roberto and Liuzza, Marco Tullio and Lombardo, Rosario and Marks, Michael J. and Meinlschmidt, Gunther and Nalborczyk, Ladislas and Nguyen, Hien T. and Ospina, Roger and Perezgonzalez, Jose D. and Pfister, Roland and Rahona, Juan J. and Rodríguez-Medina, Daniel A. and Romão, Xavier and Ruiz-Fernández, Susana and Suarez, Irina and Tegethoff, Marion and Tejo, Marcela and van de Schoot, Rens and Vankov, Ivan I. and Velasco-Forero, Santiago and Wang, Tao and Yamada, Yuki and Zoppino, Federico C. M. and Marmolejo-Ramos, Fernando},
  journal   = {Frontiers in Psychology},
  volume    = {9},
  pages     = {699},
  year      = {2018},
  month     = {May},
  doi       = {10.3389/fpsyg.2018.00699},
  pmid      = {29867666},
  pmcid     = {PMC5962803},
  publisher = {Frontiers Media SA}
}

@misc{neumann_2025,
      title={Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation}, 
      author={Terrence Neumann and Maria De-Arteaga and Sina Fazelpour},
      year={2025},
      eprint={2504.08954},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2504.08954}, 
}

@misc{anthis_2025,
      title={LLM Social Simulations Are a Promising Research Method}, 
      author={Jacy Reese Anthis and Ryan Liu and Sean M. Richardson and Austin C. Kozlowski and Bernard Koch and James Evans and Erik Brynjolfsson and Michael Bernstein},
      year={2025},
      eprint={2504.02234},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2504.02234}, 
}

@unpublished{hewitt2024predicting,
  title={Predicting Results of Social Science Experiments Using Large Language Models},
  author={Hewitt, Luke and Ashokkumar, Ashwini and Ghezae, Isaias and Willer, Robb},
  year={2024},
  month={08},
  note={Equal contribution, order randomized},
  institution={Stanford University, New York University}
}

@article{rossi_2024, title={The Problems of LLM-generated Data in Social Science Research}, volume={18}, url={https://sociologica.unibo.it/article/view/19576}, DOI={10.6092/issn.1971-8853/19576}, abstractNote={&amp;lt;p style=&amp;quot;font-weight: 400;&amp;quot;&amp;gt;Beyond being used as fast and cheap annotators for otherwise complex classification tasks, LLMs have seen a growing adoption for generating synthetic data for social science and design research. Researchers have used LLM-generated data for data augmentation and prototyping, as well as for direct analysis where LLMs acted as proxies for real human subjects. LLM-based synthetic data build on fundamentally different epistemological assumptions than previous synthetically generated data and are justified by a different set of considerations. In this essay, we explore the various ways in which LLMs have been used to generate research data and consider the underlying epistemological (and accompanying methodological) assumptions. We challenge some of the assumptions made about LLM-generated data, and we highlight the main challenges that social sciences and humanities need to address if they want to adopt LLMs as synthetic data generators.&amp;lt;/p&amp;gt;}, number={2}, journal={Sociologica}, author={Rossi, Luca and Harrison, Katherine and Shklovski, Irina}, year={2024}, month={Jan.}, pages={145–168} }

@article{jansen_2023,
title = {Employing large language models in survey research},
journal = {Natural Language Processing Journal},
volume = {4},
pages = {100020},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000171},
author = {Bernard J. Jansen and Soon-gyo Jung and Joni Salminen},
keywords = {Survey research, Large language models, Survey data, Surveys, LLM survey respondents},
abstract = {This article discusses the promising potential of employing large language models (LLMs) for survey research, including generating responses to survey items. LLMs can address some of the challenges associated with survey research regarding question-wording and response bias. They can address issues relating to a lack of clarity and understanding but cannot yet correct for sampling or nonresponse bias challenges. While LLMs can assist with some of the challenges with survey research, at present, LLMs need to be used in conjunction with other methods and approaches. With thoughtful and nuanced approaches to development, LLMs can be used responsibly and beneficially while minimizing the associated risks.}
}

@article{bisbee_2023, title={Synthetic Replacements for Human Survey Data? The Perils of Large Language Models}, volume={32}, DOI={10.1017/pan.2024.5}, number={4}, journal={Political Analysis}, author={Bisbee, James and Clinton, Joshua D. and Dorff, Cassy and Kenkel, Brenton and Larson, Jennifer M.}, year={2024}, pages={401–416}} 

@article{wilcoxon-1945,
 ISSN = {00994987},
 URL = {http://www.jstor.org/stable/3001968},
 author = {Frank Wilcoxon},
 journal = {Biometrics Bulletin},
 number = {6},
 pages = {80--83},
 publisher = {[International Biometric Society, Wiley]},
 title = {Individual Comparisons by Ranking Methods},
 urldate = {2025-07-16},
 volume = {1},
 year = {1945}
}
