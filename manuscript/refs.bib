@article{ChenFengYi2017,
  title={A general introduction to adjustment for multiple comparisons},
  author={Chen, S.Y. and Feng, Z. and Yi, X.},
  journal={Journal of Thoracic Disease},
  volume={9},
  number={6},
  pages={1725--1729},
  year={2017},
  note={doi: 10.21037/jtd.2017.05.34; PMID: 28740688; PMCID: PMC5506159},
}

@article{holms,
	ISSN = {03036898, 14679469},
	URL = {http://www.jstor.org/stable/4615733},
	abstract = {This paper presents a simple and widely applicable multiple test procedure of the sequentially rejective type, i.e. hypotheses are rejected one at a time until no further rejections can be done. It is shown that the test has a prescribed level of significance protection against error of the first kind for any combination of true hypotheses. The power properties of the test and a number of possible applications are also discussed.},
	author = {Sture Holm},
	journal = {Scandinavian Journal of Statistics},
	number = {2},
	pages = {65--70},
	publisher = {[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
	title = {A Simple Sequentially Rejective Multiple Test Procedure},
	urldate = {2025-07-07},
	volume = {6},
	year = {1979}
}

@inproceedings{pavlopoulos-likas-2024,
    title = "Polarized Opinion Detection Improves the Detection of Toxic Language",
    author = "Pavlopoulos, John  and
      Likas, Aristidis",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.117/",
    pages = "1946--1958",
    abstract = "Distance from unimodality (DFU) has been found to correlate well with human judgment for the assessment of polarized opinions. However, its un-normalized nature makes it less intuitive and somewhat difficult to exploit in machine learning (e.g., as a supervised signal). In this work a normalized version of this measure, called nDFU, is proposed that leads to better assessment of the degree of polarization. Then, we propose a methodology for K-class text classification, based on nDFU, that exploits polarized texts in the dataset. Such polarized instances are assigned to a separate K+1 class, so that a K+1-class classifier is trained. An empirical analysis on three datasets for abusive language detection, shows that nDFU can be used to model polarized annotations and prevent them from harming the classification performance. Finally, we further exploit nDFU to specify conditions that could explain polarization given a dimension and present text examples that polarized the annotators when the dimension was gender and race. Our code is available at https://github.com/ipavlopoulos/ndfu."
}

@inproceedings{sap-etal-2022-annotators,
    title = "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    author = "Sap, Maarten  and
      Swayamdipta, Swabha  and
      Vianna, Laura  and
      Zhou, Xuhui  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.431/",
    doi = "10.18653/v1/2022.naacl-main.431",
    pages = "5884--5906",
    abstract = "The perceived toxicity of language can vary based on someone{'}s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system{'}s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection."
}

@inproceedings{kumar-et-al-2021,
    author = {Kumar, Deepak and Kelley, Patrick Gage and Consolvo, Sunny and Mason, Joshua and Bursztein, Elie and Durumeric, Zakir and Thomas, Kurt and Bailey, Michael},
    title = {Designing toxic content classification for a diversity of perspectives},
    year = {2021},
    isbn = {978-1-939133-25-0},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {In this work, we demonstrate how existing classifiers for identifying toxic comments online fail to generalize to the diverse concerns of Internet users. We survey 17,280 participants to understand how user expectations for what constitutes toxic content differ across demographics, beliefs, and personal experiences. We find that groups historically at-risk of harassment--such as people who identify as LGBTQ+ or young adults--are more likely to to flag a random comment drawn from Reddit, Twitter, or 4chan as toxic, as are people who have personally experienced harassment in the past. Based on our findings, we show how current one-size-fits-all toxicity classification algorithms, like the Perspective API from Jigsaw, can improve in accuracy by 86\% on average through personalized model tuning. Ultimately, we highlight current pitfalls and new design directions that can improve the equity and efficacy of toxic content classifiers for all users.},
    booktitle = {Proceedings of the Seventeenth USENIX Conference on Usable Privacy and Security},
    articleno = {16},
    numpages = {19},
    series = {SOUPS'21}
}

@misc{tsirmpas2025scalableevaluationonlinefacilitation,
      title={Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions}, 
      author={Dimitris Tsirmpas and Ion Androutsopoulos and John Pavlopoulos},
      year={2025},
      eprint={2503.16505},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2503.16505}, 
}

@article{trafimow2018manipulating,
  title     = {Manipulating the Alpha Level Cannot Cure Significance Testing},
  author    = {Trafimow, David and Amrhein, Valentin and Areshenkoff, Cameron N. and Barrera-Causil, Carlos J. and Beh, Eric J. and Bilgiç, Yusuf Kaan and Bono, Roser and Bradley, Michael T. and Briggs, William M. and Cepeda-Freyre, Horacio A. and Chaigneau, Sergio E. and Ciocca, Daniel R. and Correa, Juan C. and Cousineau, Denis and de Boer, Menno R. and Dhar, Shashi S. and Dolgov, Igor and Gómez-Benito, Juana and Grendar, Marian and Grice, James W. and Guerrero-Gimenez, Manuel E. and Gutiérrez, Adriana and Huedo-Medina, Tania B. and Jaffe, Klaus and Janyan, Armina and Karimnezhad, Arash and Korner-Nievergelt, Fränzi and Kosugi, Kazuki and Lachmair, Martin and Ledesma, Ruben D. and Limongi, Roberto and Liuzza, Marco Tullio and Lombardo, Rosario and Marks, Michael J. and Meinlschmidt, Gunther and Nalborczyk, Ladislas and Nguyen, Hien T. and Ospina, Roger and Perezgonzalez, Jose D. and Pfister, Roland and Rahona, Juan J. and Rodríguez-Medina, Daniel A. and Romão, Xavier and Ruiz-Fernández, Susana and Suarez, Irina and Tegethoff, Marion and Tejo, Marcela and van de Schoot, Rens and Vankov, Ivan I. and Velasco-Forero, Santiago and Wang, Tao and Yamada, Yuki and Zoppino, Federico C. M. and Marmolejo-Ramos, Fernando},
  journal   = {Frontiers in Psychology},
  volume    = {9},
  pages     = {699},
  year      = {2018},
  month     = {May},
  doi       = {10.3389/fpsyg.2018.00699},
  pmid      = {29867666},
  pmcid     = {PMC5962803},
  publisher = {Frontiers Media SA}
}

@misc{neumann_2025,
      title={Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation}, 
      author={Terrence Neumann and Maria De-Arteaga and Sina Fazelpour},
      year={2025},
      eprint={2504.08954},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2504.08954}, 
}

@misc{anthis_2025,
      title={LLM Social Simulations Are a Promising Research Method}, 
      author={Jacy Reese Anthis and Ryan Liu and Sean M. Richardson and Austin C. Kozlowski and Bernard Koch and James Evans and Erik Brynjolfsson and Michael Bernstein},
      year={2025},
      eprint={2504.02234},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2504.02234}, 
}

@unpublished{hewitt2024predicting,
  title={Predicting Results of Social Science Experiments Using Large Language Models},
  author={Hewitt, Luke and Ashokkumar, Ashwini and Ghezae, Isaias and Willer, Robb},
  year={2024},
  month={08},
  note={Equal contribution, order randomized},
  institution={Stanford University, New York University}
}

@article{rossi_2024, title={The Problems of LLM-generated Data in Social Science Research}, volume={18}, url={https://sociologica.unibo.it/article/view/19576}, DOI={10.6092/issn.1971-8853/19576}, abstractNote={&amp;lt;p style=&amp;quot;font-weight: 400;&amp;quot;&amp;gt;Beyond being used as fast and cheap annotators for otherwise complex classification tasks, LLMs have seen a growing adoption for generating synthetic data for social science and design research. Researchers have used LLM-generated data for data augmentation and prototyping, as well as for direct analysis where LLMs acted as proxies for real human subjects. LLM-based synthetic data build on fundamentally different epistemological assumptions than previous synthetically generated data and are justified by a different set of considerations. In this essay, we explore the various ways in which LLMs have been used to generate research data and consider the underlying epistemological (and accompanying methodological) assumptions. We challenge some of the assumptions made about LLM-generated data, and we highlight the main challenges that social sciences and humanities need to address if they want to adopt LLMs as synthetic data generators.&amp;lt;/p&amp;gt;}, number={2}, journal={Sociologica}, author={Rossi, Luca and Harrison, Katherine and Shklovski, Irina}, year={2024}, month={Jan.}, pages={145–168} }

@article{jansen_2023,
title = {Employing large language models in survey research},
journal = {Natural Language Processing Journal},
volume = {4},
pages = {100020},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000171},
author = {Bernard J. Jansen and Soon-gyo Jung and Joni Salminen},
keywords = {Survey research, Large language models, Survey data, Surveys, LLM survey respondents},
abstract = {This article discusses the promising potential of employing large language models (LLMs) for survey research, including generating responses to survey items. LLMs can address some of the challenges associated with survey research regarding question-wording and response bias. They can address issues relating to a lack of clarity and understanding but cannot yet correct for sampling or nonresponse bias challenges. While LLMs can assist with some of the challenges with survey research, at present, LLMs need to be used in conjunction with other methods and approaches. With thoughtful and nuanced approaches to development, LLMs can be used responsibly and beneficially while minimizing the associated risks.}
}

@article{bisbee_2023, title={Synthetic Replacements for Human Survey Data? The Perils of Large Language Models}, volume={32}, DOI={10.1017/pan.2024.5}, number={4}, journal={Political Analysis}, author={Bisbee, James and Clinton, Joshua D. and Dorff, Cassy and Kenkel, Brenton and Larson, Jennifer M.}, year={2024}, pages={401–416}} 

@article{wilcoxon-1945,
 ISSN = {00994987},
 URL = {http://www.jstor.org/stable/3001968},
 author = {Frank Wilcoxon},
 journal = {Biometrics Bulletin},
 number = {6},
 pages = {80--83},
 publisher = {[International Biometric Society, Wiley]},
 title = {Individual Comparisons by Ranking Methods},
 urldate = {2025-07-16},
 volume = {1},
 year = {1945}
}

@article{Cohen_1960,
    author = {Jacob Cohen},
    title ={A Coefficient of Agreement for Nominal Scales},
    journal = {Educational and Psychological Measurement},
    volume = {20},
    number = {1},
    pages = {37-46},
    year = {1960},
    doi = {10.1177/001316446002000104},
    URL = {https://doi.org/10.1177/001316446002000104},
    eprint = {https://doi.org/10.1177/001316446002000104}
}

@article{Pavlopoulos2023,
  author    = {John Pavlopoulos and Aristidis Likas},
  title     = {Distance from Unimodality for the Assessment of Opinion Polarization},
  journal   = {Cognitive Computation},
  volume    = {15},
  number    = {2},
  pages     = {731--738},
  year      = {2023},
  doi       = {10.1007/s12559-022-10088-2},
  url       = {https://doi.org/10.1007/s12559-022-10088-2},
  issn      = {1866-9964}
}


@misc{un_hate_speech_targets,
	author    = {{United Nations}},
	title     = {Targets of Hate},
	year      = {2025},
	month     = {9},
	day       = {22},
	url       = {https://www.un.org/en/hate-speech/impact-and-prevention/targets-of-hate},
	note      = {Accessed: 2025-09-22}
}

@misc{ucdavis_hate_speech,
	author    = {Shelby Dioum},
	title     = {What Explains the Increase in Online Hate Speech?},
	year      = {2025},
	month     = {3},
	day       = {4},
	url       = {https://www.ucdavis.edu/magazine/what-explains-increase-online-hate-speech},
	note      = {Accessed: 2025-09-22},
	organization = {University of California, Davis}
}

@article{quaranto2022dog,
	title={Dog whistles, covertly coded speech, and the practices that enable them},
	author={Quaranto, Anne},
	journal={Synthese},
	volume={200},
	number={4},
	pages={330},
	year={2022},
	publisher={Springer}
}

@misc{eckman_2025_aligning,
	title={Aligning NLP Models with Target Population Perspectives using PAIR: Population-Aligned Instance Replication}, 
	author={Stephanie Eckman and Bolei Ma and Christoph Kern and Rob Chew and Barbara Plank and Frauke Kreuter},
	year={2025},
	eprint={2501.06826},
	archivePrefix={arXiv},
	primaryClass={stat.ME},
	url={https://arxiv.org/abs/2501.06826}, 
}

@article{akhtar_2020_modeling,
	author = {Akhtar, Sohail and Basile, Valerio and Patti, Viviana},
	year = {2020},
	month = {10},
	pages = {151-154},
	title = {Modeling Annotator Perspective and Polarized Opinions to Improve Hate Speech Detection},
	volume = {8},
	journal = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	doi = {10.1609/hcomp.v8i1.7473}
}

@misc{das_2024_investigating,
	title={Investigating Annotator Bias in Large Language Models for Hate Speech Detection}, 
	author={Amit Das and Zheng Zhang and Najib Hasan and Souvika Sarkar and Fatemeh Jamshidi and Tathagata Bhattacharya and Mostafa Rahgouy and Nilanjana Raychawdhary and Dongji Feng and Vinija Jain and Aman Chadha and Mary Sandage and Lauramarie Pope and Gerry Dozier and Cheryl Seals},
	year={2024},
	eprint={2406.11109},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2406.11109}, 
}

@InProceedings{petra_2022_handling,
	author="Kralj Novak, Petra
	and Scantamburlo, Teresa
	and Pelicon, Andra{\v{z}}
	and Cinelli, Matteo
	and Mozeti{\v{c}}, Igor
	and Zollo, Fabiana",
	editor="Ciucci, Davide
	and Couso, In{\'e}s
	and Medina, Jes{\'u}s
	and {\'{S}}l{\k{e}}zak, Dominik
	and Petturiti, Davide
	and Bouchon-Meunier, Bernadette
	and Yager, Ronald R.",
	title="Handling Disagreement in Hate Speech Modelling",
	booktitle="Information Processing and Management of Uncertainty in Knowledge-Based Systems",
	year="2022",
	publisher="Springer International Publishing",
	address="Cham",
	pages="681--695",
	abstract="Hate speech annotation for training machine learning models is an inherently ambiguous and subjective task. In this paper, we adopt a perspectivist approach to data annotation, model training and evaluation for hate speech classification. We first focus on the annotation process and argue that it drastically influences the final data quality. We then present three large hate speech datasets that incorporate annotator disagreement and use them to train and evaluate machine learning models. As the main point, we propose to evaluate machine learning models through the lens of disagreement by applying proper performance measures to evaluate both annotators' agreement and models' quality. We further argue that annotator agreement poses intrinsic limits to the performance achievable by models. When comparing models and annotators, we observed that they achieve consistent levels of agreement across datasets. We reflect upon our results and propose some methodological and ethical considerations that can stimulate the ongoing discussion on hate speech modelling and classification with disagreement.",
	isbn="978-3-031-08974-9"
}


@article{Uma_Fornaciari_Hovy_Paun_Plank_Poesio_2020, title={A Case for Soft Loss Functions}, volume={8}, url={https://ojs.aaai.org/index.php/HCOMP/article/view/7478}, DOI={10.1609/hcomp.v8i1.7478}, abstractNote={&lt;p class=&quot;abstract&quot;&gt;Recently, Peterson et al. provided evidence of the benefits of using probabilistic soft labels generated from crowd annotations for training a computer vision model, showing that using such labels maximizes performance of the models over unseen data. In this paper, we generalize these results by showing that training with soft labels is an effective method for using crowd annotations in several other ai tasks besides the one studied by Peterson &lt;em&gt;et al.&lt;/em&gt;, and also when their performance is compared with that of state-of-the-art methods for learning from crowdsourced data.&lt;/p&gt;}, number={1}, journal={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing}, author={Uma, Alexandra and Fornaciari, Tommaso and Hovy, Dirk and Paun, Silviu and Plank, Barbara and Poesio, Massimo}, year={2020}, month={Oct.}, pages={173-177} }

@inproceedings{casola-etal-2023-confidence,
	title = "Confidence-based Ensembling of Perspective-aware Models",
	author = "Casola, Silvia  and
	Lo, Soda Marem  and
	Basile, Valerio  and
	Frenda, Simona  and
	Cignarella, Alessandra Teresa  and
	Patti, Viviana  and
	Bosco, Cristina",
	editor = "Bouamor, Houda  and
	Pino, Juan  and
	Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.212/",
	doi = "10.18653/v1/2023.emnlp-main.212",
	pages = "3496--3507",
	abstract = "Research in the field of NLP has recently focused on the variability that people show in selecting labels when performing an annotation task. Exploiting disagreements in annotations has been shown to offer advantages for accurate modelling and fair evaluation. In this paper, we propose a strongly perspectivist model for supervised classification of natural language utterances. Our approach combines the predictions of several perspective-aware models using key information of their individual confidence to capture the subjectivity encoded in the annotation of linguistic phenomena. We validate our method through experiments on two case studies, irony and hate speech detection, in in-domain and cross-domain settings. The results show that confidence-based ensembling of perspective-aware models seems beneficial for classification performance in all scenarios. In addition, we demonstrate the effectiveness of our method with automatically extracted perspectives from annotations when the annotators' metadata are not available."
}